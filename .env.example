# OpenAI API Key
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# Anthropic API Key
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...

# Google API Key
# Get your key at: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=...

# Optional: Default model settings
# These values are used when not specified via CLI or code
DEFAULT_MODEL_PROVIDER=openai
DEFAULT_MODEL_NAME=gpt-4o-mini
DEFAULT_TEMPERATURE=0.0

# API Server Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=false
API_LOG_LEVEL=info

# Session Configuration
SESSION_TTL_HOURS=24
MODEL_CACHE_SIZE=10

# ============================================================
# Redis Configuration (Optional - for persistent sessions)
# ============================================================
# Redis connection URL for persistent session storage
# If not set, sessions are stored in-memory (lost on restart)
# Format: redis://[username:password@]host:port/database
# REDIS_URL=redis://localhost:6379/0
# For Docker: REDIS_URL=redis://redis:6379/0 (set in docker-compose.yml)

# Memory Management Configuration
# Enable/disable conversation summarization (default: true)
MEMORY_ENABLED=true

# Trigger summarization after N messages (default: 20)
MEMORY_TRIGGER_MESSAGE_COUNT=20

# Keep last N messages unsummarized (default: 10)
MEMORY_KEEP_RECENT_COUNT=10

# Provider for summarization (optional, defaults to request provider)
MEMORY_SUMMARIZATION_PROVIDER=openai

# Model for summarization (optional, defaults to gpt-4o-mini for cost efficiency)
MEMORY_SUMMARIZATION_MODEL=gpt-4o-mini

# Temperature for summarization (default: 0.0 for consistency)
MEMORY_SUMMARIZATION_TEMPERATURE=0.0

# ============================================================
# Docker Image Configuration
# ============================================================
# Configure image locations for all services
# Use these to specify private registries or different versions
#
# Python base image for API container
PYTHON_IMAGE=python:3.13-slim
#
# Redis image
REDIS_IMAGE=redis:7-alpine
#
# OpenSearch image
OPENSEARCH_IMAGE=opensearchproject/opensearch:2.11.0
#
# Phoenix tracing image
PHOENIX_IMAGE=arizephoenix/phoenix:latest
#
# Node.js base image for React UI build
NODE_IMAGE=node:20-alpine
#
# Nginx image for React UI runtime
NGINX_IMAGE=nginx:alpine

# ============================================================
# Tracing Configuration
# ============================================================
# Choose your tracing platform: "langsmith", "phoenix", or "none"
# LangSmith: Hosted solution, tight LangChain integration
# Phoenix: Open source, self-hosted, framework agnostic
TRACING_PROVIDER=phoenix

# ============================================================
# LangSmith Configuration (if TRACING_PROVIDER=langsmith)
# ============================================================
# Get your API key at: https://smith.langchain.com/settings
LANGCHAIN_API_KEY=lsv2_...

# Project name in LangSmith (optional, defaults to "default")
LANGCHAIN_PROJECT=langchain-docker

# LangSmith endpoint (optional, defaults to https://api.smith.langchain.com)
# LANGCHAIN_ENDPOINT=https://api.smith.langchain.com

# ============================================================
# Phoenix Configuration (if TRACING_PROVIDER=phoenix)
# ============================================================
# Phoenix endpoint (default: http://localhost:6006/v1/traces)
# For Docker: http://phoenix:6006/v1/traces (automatically set in docker-compose.yml)
# For local dev: http://localhost:6006/v1/traces
PHOENIX_ENDPOINT=http://localhost:6006/v1/traces

# Export traces to console for debugging (default: false)
PHOENIX_CONSOLE_EXPORT=false

# ============================================================
# AWS Bedrock Configuration
# ============================================================

# AWS Region for Bedrock (optional, defaults to us-east-1)
# If not set, boto3 will use AWS_REGION or its default region
AWS_DEFAULT_REGION=us-east-1

# Bedrock Model ARNs (comma-separated list)
# Use inference profile ARNs or foundation model ARNs
# Example: arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-5-sonnet-20241022-v2:0
BEDROCK_MODEL_ARNS=anthropic.claude-3-5-sonnet-20241022-v2:0,anthropic.claude-3-5-haiku-20241022-v1:0

# AWS Profile (optional)
# Use a specific AWS profile from ~/.aws/credentials or ~/.aws/config
# Useful when you have multiple AWS accounts or SSO profiles
AWS_PROFILE=default

# AWS Credentials (optional if using AWS CLI, profile, or IAM role)
# Only needed if not using 'aws configure', profile, or IAM role
# AWS_ACCESS_KEY_ID=your-access-key
# AWS_SECRET_ACCESS_KEY=your-secret-key

# Note: For local development, run 'aws configure' or 'aws sso login'
# to set up credentials. Boto3 will automatically use them.
# If using SSO, set AWS_PROFILE to your SSO profile name.

# ============================================================
# Database Configuration (for SQL Skill)
# ============================================================
# Database connection string for SQL skill agent
# SQLite (default, for demo): sqlite:///demo.db
# PostgreSQL: postgresql://user:password@host:port/database
DATABASE_URL=sqlite:///demo.db

# Enforce read-only mode for SQL queries (default: true)
# Only SELECT queries allowed when true
SQL_READ_ONLY=true

# ============================================================
# Jira Configuration (for Jira Skill)
# ============================================================
# Jira instance URL (e.g., https://your-instance.atlassian.net)
JIRA_URL=https://your-instance.atlassian.net

# Jira Bearer Token (Personal Access Token)
# Get your PAT at: https://id.atlassian.com/manage-profile/security/api-tokens
JIRA_BEARER_TOKEN=your-bearer-token

# Jira REST API version: "2" (default, recommended) or "3"
JIRA_API_VERSION=2

# ============================================================
# Knowledge Base / RAG Configuration
# ============================================================
# OpenSearch URL (required for Knowledge Base feature)
# For Docker: http://opensearch:9200 (automatically set in docker-compose.yml)
# For local dev: http://localhost:9200
OPENSEARCH_URL=http://localhost:9200

# OpenSearch index name for knowledge base (default: knowledge_base)
OPENSEARCH_INDEX=knowledge_base

# Embedding provider (default: openai)
EMBEDDING_PROVIDER=openai

# Embedding model (default: text-embedding-3-small, 1536 dimensions)
EMBEDDING_MODEL=text-embedding-3-small

# Document chunking settings
RAG_CHUNK_SIZE=500
RAG_CHUNK_OVERLAP=50

# Number of documents to retrieve for RAG (default: 5)
RAG_DEFAULT_TOP_K=5

# ============================================================
# Graph RAG Configuration (LlamaIndex + Neo4j)
# ============================================================
# Enable/disable Graph RAG (default: false)
# When enabled, documents are indexed in both vector store and knowledge graph
GRAPH_RAG_ENABLED=false

# Neo4j Connection
# For Docker: bolt://neo4j:7687 (automatically set in docker-compose.yml)
# For local dev: bolt://localhost:7687
NEO4J_URL=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-password

# Entity Schema (comma-separated list of entity types for extraction)
# The LLM will extract these types of entities from documents
GRAPH_RAG_ENTITIES=Person,Organization,Project,Technology,Concept,Document

# Relationship Schema (comma-separated list of relationship types)
# The LLM will identify these types of relationships between entities
GRAPH_RAG_RELATIONS=works_on,leads,member_of,uses,related_to,part_of,contains

# Neo4j Docker image (optional, for private registries)
# Note: Version 5.26+ required for LlamaIndex graph store compatibility
NEO4J_IMAGE=neo4j:5.26-community
